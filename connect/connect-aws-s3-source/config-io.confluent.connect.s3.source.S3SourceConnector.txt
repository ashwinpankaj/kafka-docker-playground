==========================
Storage
==========================
ðŸ”˜ mode

The connector's operation mode.

	 - Type: STRING
	 - Default: RESTORE_BACKUP
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ store.url

Store's connection URL, if applicable.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ topics.dir

Top level directory where data was stored to be re-ingested by Kafka.

	 - Type: STRING
	 - Default: topics
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ directory.delim

Directory delimiter pattern.

	 - Type: STRING
	 - Default: /
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ behavior.on.error

Error handling behavior setting for storage connectors. Must be configured to one of the following:

	 - Type: STRING
	 - Default: fail
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ format.bytearray.extension

Output file extension for Byte Array Format. Defaults to ``.bin``.

	 - Type: STRING
	 - Default: .bin
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ format.bytearray.separator



	 - Type: false
	 - Default: STRING
	 - Importance: String inserted between records for ByteArrayFormat. Defaults to ``System.lineSeparator()`` and may contain escape sequences like ``
``. An input record that contains the line separator looks like multiple records in the storage object output.
	 - Required: MEDIUM

ðŸ”˜ format.json.schema.enable

Enable reading of JSON messages with schema embedded.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

==========================
Connector
==========================
ðŸ”˜ record.batch.max.size

The maximum amount of records to return each time storage is polled.

	 - Type: INT
	 - Default: 200
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ schema.cache.size

The size of the schema cache used in the Avro converter.

	 - Type: INT
	 - Default: 50
	 - Importance: LOW
	 - Required: false

==========================
Partitioner
==========================
ðŸ”˜ partitioner.class

The partitioner to use when reading data to the store.

	 - Type: CLASS
	 - Default: io.confluent.connect.storage.partitioner.DefaultPartitioner
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ partition.field.name



	 - Type: false
	 - Default: LIST
	 - Importance: The name of the partitioning fields when FieldPartitioner is used.
	 - Required: MEDIUM

ðŸ”˜ path.format



	 - Type: false
	 - Default: STRING
	 - Importance: This configuration that was used to set the format of the data directories when partitioning with a TimeBasedPartitioner. For example, if you set ``path.format`` to ``'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH``, then a valid data directories would be: ``/year=2015/month=12/day=07/hour=15/``.
	 - Required: MEDIUM

==========================
Topic
==========================
ðŸ”˜ topic.regex.list



	 - Type: false
	 - Default: LIST
	 - Importance: A comma-separated list of pairs of type '<kafka topic>:<regex>' that is used to map file paths to Kafka topics. An example might be `topic1:.*\.json` to source all files ending in .json to a kafka topic named topic1. You can specify multiple of these topic:regex mappings to sending sets of files to different topics. Any files that aren't mapped by a regex will be ignored and not  sourced. The `topic.regex.list` property matches the full path (for example, `folder/file.txt`), not just the filename.
	 - Required: HIGH

==========================
Storage
==========================
ðŸ”˜ task.batch.size

The number of files assigned to each task at a time

	 - Type: INT
	 - Default: 10
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ file.discovery.starting.timestamp

A unix timestamp that denotes where to start processing files. Any file encountered with a creation time earlier than this will be ignored

	 - Type: LONG
	 - Default: 0
	 - Importance: MEDIUM
	 - Required: false

==========================
Connector
==========================
ðŸ”˜ bucket.listing.max.objects.threshold

	

	 - Type: INT
	 - Default: -1
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ parse.error.topic.prefix

Topic name prefix for topic where records with error information would be published, whenever connector encounters a malformed file.

	 - Type: STRING
	 - Default: error
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ bucket.scan.executor.threads

Number of threads to scan the bucket's folders in parallel. To get better performance , files should be distributed among multiple folders under root/topic directory. E.g. -> In case of topics->{dir1, dir2, dir3, dir4, dir5} connector will start scanning dir1, dir2, dir3, dir4, dir5 in parallel if `Bucket scan threads` is set to 5

	 - Type: INT
	 - Default: 5
	 - Importance: MEDIUM
	 - Required: false

==========================
csv
==========================
ðŸ”˜ value.schema



	 - Type: false
	 - Default: STRING
	 - Importance: The schema for the value written to Kafka.
	 - Required: MEDIUM

ðŸ”˜ csv.skip.lines

Number of lines to skip in the beginning of the file.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.separator.char

The character that separates each field in the form of an integer. Typically in a CSV this is a ,(44) character. A TSV would use a tab(9) character.

	 - Type: INT
	 - Default: 44
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.quote.char

The character that is used to quote a field. This typically happens when the csv.separator.char character is within the data.

	 - Type: INT
	 - Default: 34
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.escape.char

The character as an integer to use when a special character is encountered. The default escape character is typically a \(92)

	 - Type: INT
	 - Default: 92
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.strict.quotes

Sets the strict quotes setting - if true, characters outside the quotes are ignored.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.ignore.leading.whitespace

Sets the ignore leading whitespace setting - if true, white space in front of a quote in a field is ignored.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.ignore.quotations

Sets the ignore quotations mode - if true, quotations are ignored.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.keep.carriage.return

Flag to determine if the carriage return at the end of the line should be maintained. 

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.null.field.indicator

Indicator to determine how the CSV Reader can determine if a field is null. Valid values are [EMPTY_SEPARATORS, EMPTY_QUOTES, BOTH, NEITHER]. For more information see http://opencsv.sourceforge.net/apidocs/com/opencsv/enums/CSVReaderNullFieldIndicator.html.

	 - Type: STRING
	 - Default: NEITHER
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.first.row.as.header

Flag to indicate if the fist row of data contains the header of the file. If true the position of the columns will be determined by the first row to the CSV. The column position will be inferred from the position of the schema supplied in `value.schema`. If set to true the number of columns must be greater than or equal to the number of fields in the schema.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ csv.file.charset

Character set to read wth file with.

	 - Type: STRING
	 - Default: UTF-8
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.case.sensitive.field.names

Flag to determine if the field names in the header row should be treated as case sensitive.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.rfc.4180.parser.enabled

Flag to determine if the RFC 4180 parser should be used instead of the default parser.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

==========================
folders
==========================
ðŸ”˜ LIST



	 - Type: HIGH
	 - Default: false
	 - Importance: 
	 - Required: 

==========================
filename.regex
==========================
ðŸ”˜ (.+)\+(\d+)\+.+$



	 - Type: false
	 - Default: STRING
	 - Importance: 
	 - Required: MEDIUM

==========================
Connector
==========================
ðŸ”˜ s3.poll.interval.ms

Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.

	 - Type: LONG
	 - Default: 60000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ format.class

Class responsible for converting storage objects to source records.

	 - Type: CLASS
	 - Default: null
	 - Importance: HIGH
	 - Required: true

==========================
S3
==========================
ðŸ”˜ s3.bucket.name

The S3 bucket to read records from.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ s3.region

The AWS region to be used by the connector.

	 - Type: STRING
	 - Default: us-west-2
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ s3.credentials.provider.class

Credentials provider or provider chain to use for authentication to AWS. By default the connector uses 'DefaultCredentialsProvider'.

	 - Type: CLASS
	 - Default: software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider
	 - Importance: LOW
	 - Required: false

ðŸ”˜ aws.access.key.id



	 - Type: false
	 - Default: STRING
	 - Importance: The AWS Access Key ID used to connect to S3. If specified, this will be used over the credentials provider chain.
	 - Required: MEDIUM

ðŸ”˜ aws.secret.access.key

The AWS Secret Access Key used to connect to S3. If specified, this will be used over the credentials provider chain.

	 - Type: PASSWORD
	 - Default: [hidden]
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ s3.part.retries

 Maximum number of retry attempts for failed requests. Zero means no retries. The actual number of attempts is determined by the S3 client based on multiple factors, including, but not limited to the value of this parameter, the type of exception that occurred, throttling settings of the underlying S3 client, and etc.

	 - Type: INT
	 - Default: 3
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ s3.retry.backoff.ms

How long to wait in milliseconds before attempting the first retry of a failed S3 request. Upon a failure, this connector may wait up to twice as long as the previous wait, up to the maximum number of retries. This avoids retrying in a tight loop under failure scenarios.

	 - Type: INT
	 - Default: 200
	 - Importance: LOW
	 - Required: false

ðŸ”˜ s3.wan.mode

Use the S3 accelerated endpoint.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ s3.path.style.access

Whether to use s3 path-style access.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ s3.http.send.expect.continue

Enable/disable use of the HTTP/1.1 handshake using EXPECT: 100-CONTINUE during multi-part upload. If true, the client waits for a 100 (CONTINUE) response before sending the request body. If false, the client uploads the entire request body without checking if the server is willing to accept the request.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ s3.ssea.name



	 - Type: false
	 - Default: STRING
	 - Importance: The S3 server-side encryption algorithm.
	 - Required: LOW

ðŸ”˜ s3.sse.customer.key

The S3 Server-Side Encryption customer-provided key (SSE-C).

	 - Type: PASSWORD
	 - Default: null
	 - Importance: LOW
	 - Required: false

==========================
Proxy
==========================
ðŸ”˜ s3.proxy.url



	 - Type: false
	 - Default: STRING
	 - Importance: S3 Proxy settings encoded in URL syntax. This property is meant to be used only if you need to access S3 through a proxy.
	 - Required: LOW

ðŸ”˜ s3.proxy.username

S3 Proxy User. This property is meant to be used only if you need to access S3 through a proxy. Using ``s3.proxy.username`` instead of embedding the username and password in ``s3.proxy.url`` allows the password to be hidden in the logs.

	 - Type: STRING
	 - Default: null
	 - Importance: LOW
	 - Required: false

ðŸ”˜ s3.proxy.password

S3 Proxy Password. This property is meant to be used only if you need to access S3 through a proxy. Using ``s3.proxy.password`` instead of embedding the username and password in ``s3.proxy.url`` allows the password to be hidden in the logs.

	 - Type: PASSWORD
	 - Default: null
	 - Importance: LOW
	 - Required: false

==========================
Serialization
==========================
ðŸ”˜ value.converter.decimal.format

Controls which format this converter will serialize decimals in. This value is case insensitive and can be either 'BASE64' (default) or 'NUMERIC'

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

==========================
null
==========================
ðŸ”˜ value.converter.converter.type

How this converter will be used.

	 - Type: STRING
	 - Default: null
	 - Importance: LOW
	 - Required: true

==========================
Schemas
==========================
ðŸ”˜ value.converter.schemas.cache.size

The maximum number of schemas that can be cached in this converter instance.

	 - Type: INT
	 - Default: 1000
	 - Importance: HIGH
	 - Required: false

==========================
Serialization
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

==========================
Schemas
==========================
ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values and keys.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: HIGH
	 - Required: false

