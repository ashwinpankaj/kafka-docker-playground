==========================
General
==========================
ðŸ”˜ topic

The Kafka topic to write the data to.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ batch.size

The number of records that should be returned with each batch.

	 - Type: INT
	 - Default: 1000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ empty.poll.wait.ms

The amount of time to wait if a poll returns an empty list of records.

	 - Type: LONG
	 - Default: 500
	 - Importance: LOW
	 - Required: false

==========================
File System
==========================
ðŸ”˜ cleanup.policy

Determines how the connector should cleanup the files that have been successfully processed. NONE leaves the files in place which could cause them to be reprocessed if the connector is restarted. DELETE removes the file from the filesystem. MOVE will move the file to a finished directory. MOVEBYDATE will move the file to a finished directory with subdirectories by date

	 - Type: STRING
	 - Default: MOVE
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ input.path

The directory to read files that will be processed. This directory must exist and be writable by the user running Kafka Connect.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ finished.path



	 - Type: false
	 - Default: STRING
	 - Importance: The directory to place files that have been successfully processed. This directory must exist and be writable by the user running Kafka Connect.
	 - Required: HIGH

ðŸ”˜ error.path

The directory to place files in which have error(s). This directory must exist and be writable by the user running Kafka Connect.

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ input.file.pattern

Regular expression to check input file names against. This expression must match the entire filename. The equivalent of Matcher.matches().

	 - Type: STRING
	 - Default: null
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ halt.on.error

Should the task halt when it encounters an error or continue to the next file.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ file.minimum.age.ms

The amount of time in milliseconds after the file was last written to before the file can be processed.

	 - Type: LONG
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ processing.file.extension

Before a file is processed, a flag is created in its directory to indicate the file is being handled. The flag file has the same name as the file, but with this property appended as a suffix.

	 - Type: STRING
	 - Default: .PROCESSING
	 - Importance: LOW
	 - Required: false

==========================
Timestamps
==========================
ðŸ”˜ timestamp.mode

Determines how the connector will set the timestamp for the [ConnectRecord](https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/connector/ConnectRecord.html#timestamp()). If set to `Field` then the timestamp will be read from a field in the value. This field cannot be optional and must be a [Timestamp](https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/data/Schema.html). Specify the field  in `timestamp.field`. If set to `FILE_TIME` then the last modified time of the file will be used. If set to `PROCESS_TIME` the time the record is read will be used.

	 - Type: STRING
	 - Default: PROCESS_TIME
	 - Importance: MEDIUM
	 - Required: false

==========================
File System
==========================
ðŸ”˜ files.sort.attributes

The attributes each file will use to determine the sort order. `Name` is name of the file. `Length` is the length of the file preferring larger files first. `LastModified` is the LastModified attribute of the file preferring older files first.

	 - Type: LIST
	 - Default: NameAsc
	 - Importance: LOW
	 - Required: false

==========================
General
==========================
ðŸ”˜ task.index

Internal setting to the connector used to instruct a task on which files to select. The connector will override this setting.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ task.count

Internal setting to the connector used to instruct a task on which files to select. The connector will override this setting.

	 - Type: INT
	 - Default: 1
	 - Importance: LOW
	 - Required: false

==========================
File System
==========================
ðŸ”˜ task.partitioner

The task partitioner implementation is used when the connector is configured to use more than one task. This is used by each task to identify which files will be processed by that task. This ensures that each file is only assigned to one task.

	 - Type: STRING
	 - Default: ByName
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ input.path.walk.recursively

If enabled, any sub-directories dropped under `input.path` will be recursively walked looking for files matching the configured `input.file.pattern`. After processing is complete the discovered sub directory structure (as well as files within them) will handled according to the configured `cleanup.policy` (i.e. moved or deleted etc). For each discovered file, the walked sub-directory path will be set as a header named `file.relative.path`

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ cleanup.policy.maintain.relative.path

If `input.path.walk.recursively` is enabled in combination with this flag being `true`, the walked sub-directories which contained files will be retained as-is under the `input.path`. The actual files within the sub-directories will moved (with a copy of the sub-dir structure) or deleted as per the `cleanup.policy` defined, but the parent sub-directory structure will remain.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ file.buffer.size.bytes

The size of buffer for the BufferedInputStream that will be used to interact with the file system.

	 - Type: INT
	 - Default: 131072
	 - Importance: LOW
	 - Required: false

==========================
Schema
==========================
ðŸ”˜ key.schema



	 - Type: false
	 - Default: STRING
	 - Importance: The schema for the key written to Kafka.
	 - Required: HIGH

ðŸ”˜ value.schema



	 - Type: false
	 - Default: STRING
	 - Importance: The schema for the value written to Kafka.
	 - Required: HIGH

==========================
Schema Generation
==========================
ðŸ”˜ schema.generation.enabled

Flag to determine if schemas should be dynamically generated. If set  to true, `key.schema` and `value.schema` can be omitted, but `schema.generation.key.name` and `schema.generation.value.name` must be set.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ schema.generation.key.fields



	 - Type: false
	 - Default: LIST
	 - Importance: The field(s) to use to build a key schema. This is only used during schema generation.
	 - Required: MEDIUM

ðŸ”˜ schema.generation.key.name

The name of the generated key schema.

	 - Type: STRING
	 - Default: com.github.jcustenborder.kafka.connect.model.Key
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ schema.generation.value.name

The name of the generated value schema.

	 - Type: STRING
	 - Default: com.github.jcustenborder.kafka.connect.model.Value
	 - Importance: MEDIUM
	 - Required: false

==========================
Timestamps
==========================
ðŸ”˜ parser.timestamp.timezone

The timezone that all of the dates will be parsed with.

	 - Type: STRING
	 - Default: UTC
	 - Importance: LOW
	 - Required: false

ðŸ”˜ parser.timestamp.date.formats

The date formats that are expected in the file. This is a list of strings that will be used to parse the date fields in order. The most accurate date format should be the first in the list. Take a look at the Java documentation for more info. https://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html

	 - Type: LIST
	 - Default: yyyy-MM-dd'T'HH:mm:ss,yyyy-MM-dd' 'HH:mm:ss
	 - Importance: LOW
	 - Required: false

ðŸ”˜ timestamp.field



	 - Type: false
	 - Default: STRING
	 - Importance: The field in the value schema that will contain the parsed timestamp for the record. This field cannot be marked as optional and must be a [Timestamp](https://kafka.apache.org/0102/javadoc/org/apache/kafka/connect/data/Schema.html)
	 - Required: MEDIUM

==========================
CSV Parsing
==========================
ðŸ”˜ csv.skip.lines

Number of lines to skip in the beginning of the file.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.separator.char

The character that separates each field in the form of an integer. Typically in a CSV this is a ,(44) character. A TSV would use a tab(9) character. If `csv.separator.char` is defined as a null(0), then the RFC 4180 parser must be utilized by default. This is the equivalent of `csv.rfc.4180.parser.enabled = true`.

	 - Type: INT
	 - Default: 44
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.quote.char

The character that is used to quote a field. This typically happens when the csv.separator.char character is within the data.

	 - Type: INT
	 - Default: 34
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.escape.char

The character as an integer to use when a special character is encountered. The default escape character is typically a \(92)

	 - Type: INT
	 - Default: 92
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.strict.quotes

Sets the strict quotes setting - if true, characters outside the quotes are ignored.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.ignore.leading.whitespace

Sets the ignore leading whitespace setting - if true, white space in front of a quote in a field is ignored.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.ignore.quotations

Sets the ignore quotations mode - if true, quotations are ignored.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.keep.carriage.return

Flag to determine if the carriage return at the end of the line should be maintained.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.verify.reader

Flag to determine if the reader should be verified.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.null.field.indicator

Indicator to determine how the CSV Reader can determine if a field is null. Valid values are EMPTY_SEPARATORS, EMPTY_QUOTES, BOTH, NEITHER. For more information see http://opencsv.sourceforge.net/apidocs/com/opencsv/enums/CSVReaderNullFieldIndicator.html.

	 - Type: STRING
	 - Default: NEITHER
	 - Importance: LOW
	 - Required: false

ðŸ”˜ csv.first.row.as.header

Flag to indicate if the fist row of data contains the header of the file. If true the position of the columns will be determined by the first row to the CSV. The column position will be inferred from the position of the schema supplied in `value.schema`. If set to true the number of columns must be greater than or equal to the number of fields in the schema.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ csv.file.charset

Character set to read wth file with.

	 - Type: STRING
	 - Default: UTF-8
	 - Importance: LOW
	 - Required: false

==========================
csv.case.sensitive.field.names
==========================
ðŸ”˜ false



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Flag to determine if the field names in the header row should be treated as case sensitive.
	 - Required: LOW

==========================
csv.rfc.4180.parser.enabled
==========================
ðŸ”˜ false



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Flag to determine if the RFC 4180 parser should be used instead of the default parser.
	 - Required: LOW

