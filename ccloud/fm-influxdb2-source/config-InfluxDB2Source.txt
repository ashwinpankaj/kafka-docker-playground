==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ datapreview.schemas.enable

This config key only applies to data preview requests and governs whether the data preview output has record schema with it.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
InfluxDB
==========================
ðŸ”˜ influxdb.url



	 - Type: true
	 - Default: STRING
	 - Importance: Fully qualified InfluxDB API URL used for establishing connection.
	 - Required: HIGH

ðŸ”˜ influxdb.token



	 - Type: true
	 - Default: PASSWORD
	 - Importance: Token to authenticate with influx db.
	 - Required: HIGH

ðŸ”˜ influxdb.org.id



	 - Type: true
	 - Default: STRING
	 - Importance: Organization ID.
	 - Required: HIGH

ðŸ”˜ influxdb.bucket



	 - Type: true
	 - Default: STRING
	 - Importance: Bucket from which this connector will read the data from.
	 - Required: MEDIUM

==========================
Read Configuration
==========================
ðŸ”˜ query



	 - Type: false
	 - Default: STRING
	 - Importance: If specified, this query will be executed and the resultant records will be pushed to desired Apache Kafka topic. Use this setting if there's a need to select subset of fields or tags, perform aggregations or filter data. The query should follow the template - ``import "influxdata/influxdb/schema" from(bucket:$influxdb.bucket) |> range(start: $startTimestamp, stop: $endTimestamp) |> <Your custom query criteria here> |> schema.fieldsAsCols() |> limit(n: $batch.size)`` In case of ``mode=bulk``, the connector will run the query as-is each time it polls. The range criteria should be filled in by the user. Flux does not allow unbounded queries as they are resource intensive. If you use ``mode=timestamp``, the values for ``$startTimestamp`` and ``$endTimestamp`` will be filled by the connector with appropriate source offsets.Users should replace other criteria mentioned at - ``<Your custom query criteria here>``. The connector will replace ``$influxdb.bucket`` and ``$batch.size`` with the values from the corresponding configurations.
	 - Required: MEDIUM

ðŸ”˜ mode

The mode in which measurements in InfluxDB has to be polled. Supported modes are : `bulk` performs a bulk load of the entire measurement to desired Apache Kafka topic, each time it is polled. `timestamp` uses the timestamp to detect newly created rows and writes them to desired Apache Kafka topic.

	 - Type: STRING
	 - Default: timestamp
	 - Importance: MEDIUM
	 - Required: true

ðŸ”˜ topic.mapper

Configuration to decide how to map topics Supported options are : `bucket` - Topic name is Topic Prefix + Bucket name. All the records go into same topic. Or `measurement` - Topic name is Topic Prefix + Measurement name. All the records from same measurement go into same topic.

	 - Type: STRING
	 - Default: bucket
	 - Importance: MEDIUM
	 - Required: true

ðŸ”˜ topic.prefix



	 - Type: true
	 - Default: STRING
	 - Importance: Prefix that should be prepended to measurement names to determine the name of the Apache Kafka topic to publish data to, in case of custom query, it should be the full name of the Apache Kafka topic.
	 - Required: MEDIUM

ðŸ”˜ batch.size

Maximum number of points to include in a single batch when polling for new data. This setting can be used to limit the amount of data buffered internally in the connector.

	 - Type: INT
	 - Default: 5000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ timestamp.delay.interval.ms

How long to wait after a record with certain timestamp appears before we include it in the result. You may choose to add some delay to allow transactions with earlier timestamp to complete. The first execution will fetch all available records (i.e. starting at Unix Epoch) until current time minus the delay. Every following execution will get data from the time of the last record fetched in the previous batch until current time minus the delay.

	 - Type: INT
	 - Default: 0
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ influxdb.measurement.whitelist



	 - Type: false
	 - Default: STRING
	 - Importance: Comma separated list of measurements to include in copying. If specified, Measurements Excluded cannot be set. If left empty, all measurements will be included.
	 - Required: MEDIUM

ðŸ”˜ influxdb.measurement.blacklist



	 - Type: false
	 - Default: STRING
	 - Importance: Comma separated list of measurements to exclude from copying. If specified, Measurements Included cannot be set.
	 - Required: MEDIUM

==========================
Retries
==========================
ðŸ”˜ retry.backoff.ms

Backoff time duration to wait before retrying

	 - Type: INT
	 - Default: 1000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ max.retries

The maximum number of times to retry on errors before failing the task.

	 - Type: INT
	 - Default: 10
	 - Importance: MEDIUM
	 - Required: false

==========================
Output messages
==========================
ðŸ”˜ output.data.format

Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

