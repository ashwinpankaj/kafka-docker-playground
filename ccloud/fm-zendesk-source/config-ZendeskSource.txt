==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ datapreview.schemas.enable

This config key only applies to data preview requests and governs whether the data preview output has record schema with it.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
How do you want to name your topic(s)?
==========================
ðŸ”˜ topic.name.pattern

The pattern to use for the topic name, where the ``${entityName}`` literal will be replaced with each entity name. If ``${entityName}`` is not specified all the records will be written to a single topic. A valid topic pattern should follow the regex [a-zA-Z0-9\.\-\_]*(\$\{entityName\})?[a-zA-Z0-9\.\-\_]*

	 - Type: STRING
	 - Default: ZD_${entityName}
	 - Importance: HIGH
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
How should we connect to Zendesk?
==========================
ðŸ”˜ zendesk.auth.type

Authentication type of the endpoint. Valid values are ``basic`` and ``bearer``

	 - Type: STRING
	 - Default: basic
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ zendesk.url



	 - Type: true
	 - Default: STRING
	 - Importance: The zendesk service url that connector will connect to.
	 - Required: HIGH

ðŸ”˜ zendesk.tables



	 - Type: true
	 - Default: LIST
	 - Importance: The Zendesk tables that are to be exported and written to Kafka. To avail a reasonable load balance between workers, the tables could be ordered by their expected size or throughput.
	 - Required: HIGH

ðŸ”˜ zendesk.since



	 - Type: false
	 - Default: STRING
	 - Importance: Rows updated after this time will be processed by the connector. If left blank, the default time will be set to the time this connector is launched minus 1 minute. The value should be formatted as ISO 8601. Example format yyyy-MM-dd'T'HH:mm:SS.
	 - Required: MEDIUM

==========================
Authorization: Basic
==========================
ðŸ”˜ zendesk.user



	 - Type: false
	 - Default: STRING
	 - Importance: The username to be used with an endpoint requiring authentication.
	 - Required: HIGH

ðŸ”˜ zendesk.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The password to be used with an endpoint requiring authentication.
	 - Required: HIGH

==========================
Authorization: Bearer
==========================
ðŸ”˜ bearer.token



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The bearer authentication token to be used when ``auth.type=bearer``. The supplied token will be used as the value of ``Authorization`` header in HTTP requests.
	 - Required: HIGH

==========================
Connection details
==========================
ðŸ”˜ max.batch.size

The maximum number of records that should be returned and written to Kafka at one time.

	 - Type: INT
	 - Default: 100
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.in.flight.requests

The maximum number of requests that may be in-flight at once.

	 - Type: INT
	 - Default: 10
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.poll.interval.ms

The time in milliseconds between requests to fetch changed or updated entities.

	 - Type: LONG
	 - Default: 3000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ request.interval.ms

The time in milliseconds to wait before checking for updated records.

	 - Type: LONG
	 - Default: 15000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.retries

The maximum number of times to retry on errors before failing the task.

	 - Type: INT
	 - Default: 10
	 - Importance: LOW
	 - Required: false

ðŸ”˜ retry.backoff.ms

The time in milliseconds to wait following an error before a retry attempt is made.

	 - Type: LONG
	 - Default: 3000
	 - Importance: LOW
	 - Required: false

==========================
Output messages
==========================
ðŸ”˜ output.data.format

Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

