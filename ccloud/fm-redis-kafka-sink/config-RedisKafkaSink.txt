==========================
Which topics do you want to get data from?
==========================
ðŸ”˜ topics.regex



	 - Type: false
	 - Default: STRING
	 - Importance: A regular expression that matches the names of the topics to consume from. This is useful when you want to consume from multiple topics that match a certain pattern without having to list them all individually.
	 - Required: LOW

ðŸ”˜ topics



	 - Type: true
	 - Default: LIST
	 - Importance: Identifies the topic name or a comma-separated list of topic names.
	 - Required: HIGH

ðŸ”˜ errors.deadletterqueue.topic.name

The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. Defaults to 'dlq-${connector}' if not set. The DLQ topic will be created automatically if it does not exist. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: dlq-${connector}
	 - Importance: LOW
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

==========================
CSFLE
==========================
ðŸ”˜ csfle.enabled

Determines whether the connector honours CSFLE rules or not

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ sr.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry.
	 - Required: HIGH

ðŸ”˜ csfle.onFailure

Configures the behavior for decryption failures. If set to ERROR, the connector will behave as configured for error behaviour. If set to NONE, the connector will ignore the decryption failure and proceed to write the data in its encrypted form.

	 - Type: STRING
	 - Default: ERROR
	 - Importance: MEDIUM
	 - Required: false

==========================
Redis connection
==========================
ðŸ”˜ redis.host



	 - Type: true
	 - Default: STRING
	 - Importance: The hostname of Redis server to connect to.
	 - Required: HIGH

ðŸ”˜ redis.port



	 - Type: true
	 - Default: STRING
	 - Importance: The port number of Redis server to connect to.
	 - Required: HIGH

ðŸ”˜ redis.database

The database index to write to.

	 - Type: INT
	 - Default: 0
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ redis.username



	 - Type: false
	 - Default: STRING
	 - Importance: The username of the Redis user connecting to the Redis database server.
	 - Required: MEDIUM

ðŸ”˜ redis.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The password of the Redis user connecting to the Redis database server.
	 - Required: MEDIUM

==========================
Redis security
==========================
ðŸ”˜ redis.tls

Establish a secure TLS connection to Redis.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ redis.cacert



	 - Type: false
	 - Default: PASSWORD
	 - Importance: X.509 CA certificate file to verify with. Use this with or without client certificates.
	 - Required: MEDIUM

==========================
Redis client certificate auth
==========================
ðŸ”˜ redis.key.file



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Private key file (PEM format) to authenticate with. Use this file along with the certificate file for client certificate authentication.
	 - Required: MEDIUM

ðŸ”˜ redis.key.cert



	 - Type: false
	 - Default: PASSWORD
	 - Importance: X.509 certificate chain file (PEM format) to authenticate with. Use this file along with the private key file for client certificate authentication.
	 - Required: MEDIUM

ðŸ”˜ redis.key.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Password of the private key file. Leave empty if key file is not password-protected.
	 - Required: MEDIUM

==========================
Redis data structure
==========================
ðŸ”˜ redis.type

Destination Redis data structure. Valid entries are STREAM, LIST, SET, ZSET, STRING, HASH, TIMESERIES, or JSON. Note that multi/exec transactions only work with STREAM, LIST, SET, or ZSET data types.

	 - Type: STRING
	 - Default: STREAM
	 - Importance: HIGH
	 - Required: true

==========================
Input messages
==========================
ðŸ”˜ input.data.format

Sets the input Kafka record value format. Valid entries are AVRO, BYTES, JSON, JSON_SR, PROTOBUF, or STRING. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ input.key.format

Sets the input Kafka record key format. Valid entries are AVRO, BYTES, JSON, JSON_SR, PROTOBUF, STRING. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: BYTES
	 - Importance: HIGH
	 - Required: false

==========================
Redis configuration
==========================
ðŸ”˜ redis.keyspace

A format string for destination key space, which may contain ``${topic}`` as a placeholder for the originating topic name. For example, ``kafka_${topic}`` for the topic ``orders`` will map to the Redis key space ``kafka_orders``.

	 - Type: STRING
	 - Default: ${topic}
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ redis.separator

Separator for non-collection destination keys.

	 - Type: STRING
	 - Default: :
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ redis.key.ttl

Time to live in seconds for the key to automatically expire data after a specified duration. If not set (default ``-1``), the record will not expire.

	 - Type: LONG
	 - Default: -1
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ redis.multiexec

Whether to execute Redis commands in multi/exec transactions. Only works with STREAM, LIST, SET, or ZSET data types.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ redis.wait.replicas

Number of replicas to wait for. Use ``0`` to disable waiting for replicas.

	 - Type: INT
	 - Default: 0
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ redis.charset

Character set to encode Redis key and value strings. Must be a valid charset name.

	 - Type: STRING
	 - Default: UTF-8
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ redis.wait.timeout

Timeout in milliseconds for WAIT command.

	 - Type: LONG
	 - Default: 1000
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ redis.timeout

Redis command timeout in seconds.

	 - Type: LONG
	 - Default: 60
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ redis.pool

Maximum number of connections in the pool in the range of 1 to 100.

	 - Type: INT
	 - Default: 8
	 - Importance: MEDIUM
	 - Required: false

ðŸ”˜ redis.server.mode

Whether redis server is running on one or multiple nodes.

	 - Type: STRING
	 - Default: Standalone
	 - Importance: MEDIUM
	 - Required: false

==========================
Consumer configuration
==========================
ðŸ”˜ max.poll.interval.ms

The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).

	 - Type: LONG
	 - Default: 300000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.poll.records

The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.

	 - Type: LONG
	 - Default: 500
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: all
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

