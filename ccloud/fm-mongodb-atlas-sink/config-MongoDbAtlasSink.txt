==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Input messages
==========================
ðŸ”˜ input.data.format

Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, JSON, STRING or BSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.

	 - Type: STRING
	 - Default: JSON
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ cdc.handler

The class name of the CDC handler to use for processing. You can capture CDC events with the MongoDB Kafka sink connector and perform corresponding insert, update, and delete operations to a destination MongoDB cluster.

	 - Type: STRING
	 - Default: None
	 - Importance: LOW
	 - Required: false

ðŸ”˜ input.key.format

Sets the input Kafka record key format. Valid entries are AVRO, BYTES, JSON, JSON_SR, PROTOBUF, or STRING. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF

	 - Type: STRING
	 - Default: STRING
	 - Importance: HIGH
	 - Required: false

==========================
Writes
==========================
ðŸ”˜ delete.on.null.values

Whether or not the connector should try to delete documents based on key when value is null.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.batch.size

The maximum number of sink records to possibly batch together for processing.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ bulk.write.ordered

Whether the batches controlled by 'max.batch.size' must be written via ordered bulk writes.

	 - Type: STRING
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ rate.limiting.timeout

How long in ms processing should wait before continuing after triggering a rate limit.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ rate.limiting.every.n

The number of processed batches that will trigger rate limiting. The default value of 0 sets no rate limiting.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ write.strategy

The class that specifies the WriteModel to use for bulk writes.

	 - Type: STRING
	 - Default: DefaultWriteModelStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ delete.write.strategy

The class that handles how to build the delete write models for the sink documents.

	 - Type: STRING
	 - Default: DeleteOneDefaultStrategy
	 - Importance: LOW
	 - Required: false

==========================
CSFLE
==========================
ðŸ”˜ csfle.enabled

Determines whether the connector honours CSFLE rules or not

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ sr.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry.
	 - Required: HIGH

ðŸ”˜ csfle.onFailure

Configures the behavior for decryption failures. If set to ERROR, the connector will behave as configured for error behaviour. If set to NONE, the connector will ignore the decryption failure and proceed to write the data in its encrypted form.

	 - Type: STRING
	 - Default: ERROR
	 - Importance: MEDIUM
	 - Required: false

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

==========================
Which topics do you want to get data from?
==========================
ðŸ”˜ topics.regex



	 - Type: false
	 - Default: STRING
	 - Importance: A regular expression that matches the names of the topics to consume from. This is useful when you want to consume from multiple topics that match a certain pattern without having to list them all individually.
	 - Required: LOW

ðŸ”˜ topics



	 - Type: true
	 - Default: LIST
	 - Importance: Identifies the topic name or a comma-separated list of topic names.
	 - Required: HIGH

ðŸ”˜ errors.deadletterqueue.topic.name

The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. Defaults to 'dlq-${connector}' if not set. The DLQ topic will be created automatically if it does not exist. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: dlq-${connector}
	 - Importance: LOW
	 - Required: false

==========================
How should we connect to your MongoDB database?
==========================
ðŸ”˜ mongodb.instance.type

Specifies the type of MongoDB instance the connector will connect to.

	 - Type: STRING
	 - Default: MONGODB_ATLAS
	 - Importance: HIGH
	 - Required: true

ðŸ”˜ mongodb.auth.mechanism

Choose an authentication mechanism for MongoDB. Use SCRAM-SHA-256 for username/password authentication. Use MONGODB-X509 for certificate-based authentication. For MONGODB-X509, you must configure the SSL keystore properties.

	 - Type: STRING
	 - Default: SCRAM-SHA-256
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ connection.host



	 - Type: true
	 - Default: STRING
	 - Importance: For MongoDB Atlas, provide the SRV connection host (e.g., mycluster.abc123.mongodb.net). For Self Managed MongoDB, provide the host and port in MongoDB URI format, e.g., host1:27017 or host1:27017/?replicaSet=myReplicaSet.
	 - Required: HIGH

ðŸ”˜ connection.user



	 - Type: false
	 - Default: STRING
	 - Importance: MongoDB connection user.
	 - Required: HIGH

ðŸ”˜ connection.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: MongoDB connection password.
	 - Required: HIGH

ðŸ”˜ connection.ssl.truststore.file



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The trust store file containing trusted certificates. Supported formats include JKS and PKCS12. If not set, the default Java trust store is used.
	 - Required: MEDIUM

ðŸ”˜ connection.ssl.truststorePassword



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The password for the trust store file.
	 - Required: MEDIUM

ðŸ”˜ connection.ssl.keystore.file



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The key store file containing the client certificate and private key for MONGODB-X509 authentication. Supported formats include JKS and PKCS12.
	 - Required: MEDIUM

ðŸ”˜ connection.ssl.keystorePassword



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The password for the key store file. This is optional for the client and only needed if ``connection.ssl.keystore.file`` is configured.
	 - Required: MEDIUM

ðŸ”˜ database



	 - Type: false
	 - Default: STRING
	 - Importance: MongoDB database name.
	 - Required: HIGH

==========================
Database details
==========================
ðŸ”˜ collection



	 - Type: false
	 - Default: STRING
	 - Importance: MongoDB collection name.
	 - Required: MEDIUM

==========================
ID strategies
==========================
ðŸ”˜ doc.id.strategy

The IdStrategy class name to use for generating a unique document id (_id).

	 - Type: STRING
	 - Default: BsonOidStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ doc.id.strategy.overwrite.existing

Whether the connector should overwrite existing values in the `_id` field when the strategy defined in doc.id.strategy is applied.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ document.id.strategy.uuid.format

The bson output format when using the `UuidStrategy`. Can be either `String` or `Binary`.

	 - Type: STRING
	 - Default: string
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.projection.type

For use with the `PartialKeyStrategy` allows custom key fields to be projected for the ID strategy. Use either `AllowList` or `BlockList`.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.projection.list



	 - Type: false
	 - Default: STRING
	 - Importance: For use with the `PartialKeyStrategy` allows custom key fields to be projected for the ID strategy. A comma-separated list of field names for key projection.
	 - Required: LOW

ðŸ”˜ value.projection.type

For use with the `PartialValueStrategy` allows custom value fields to be projected for the ID strategy. Use either `AllowList` or `BlockList`.

	 - Type: STRING
	 - Default: none
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.projection.list



	 - Type: false
	 - Default: STRING
	 - Importance: For use with the `PartialValueStrategy` allows custom value fields to be projected for the ID strategy. A comma-separated list of field names for value projection.
	 - Required: LOW

==========================
Namespace mapping
==========================
ðŸ”˜ namespace.mapper.class

The class that determines the namespace to write the sink data to. By default this will be based on the 'database' configuration and either the topic name or the 'collection' configuration.

	 - Type: STRING
	 - Default: DefaultNamespaceMapper
	 - Importance: LOW
	 - Required: false

ðŸ”˜ namespace.mapper.key.database.field



	 - Type: false
	 - Default: STRING
	 - Importance: The key field to use as the destination database name.
	 - Required: LOW

ðŸ”˜ namespace.mapper.key.collection.field



	 - Type: false
	 - Default: STRING
	 - Importance: The key field to use as the destination collection name.
	 - Required: LOW

ðŸ”˜ namespace.mapper.value.database.field



	 - Type: false
	 - Default: STRING
	 - Importance: The value field to use as the destination database name.
	 - Required: LOW

ðŸ”˜ namespace.mapper.value.collection.field



	 - Type: false
	 - Default: STRING
	 - Importance: The value field to use as the destination collection name.
	 - Required: LOW

ðŸ”˜ namespace.mapper.error.if.invalid

Whether to throw an error if the mapped field is missing or invalid. Defaults to false.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Server API
==========================
ðŸ”˜ server.api.version



	 - Type: false
	 - Default: STRING
	 - Importance: The server API version to use. Disabled by default.
	 - Required: LOW

ðŸ”˜ server.api.deprecation.errors

Sets whether the connector requires use of deprecated server APIs to be reported as errors.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ server.api.strict

Sets whether the application requires strict server API version enforcement.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

==========================
Connection details
==========================
ðŸ”˜ max.num.retries

How many retries should be attempted on write errors.

	 - Type: INT
	 - Default: 3
	 - Importance: LOW
	 - Required: false

ðŸ”˜ retries.defer.timeout

How long a retry should get deferred.

	 - Type: INT
	 - Default: 5000
	 - Importance: LOW
	 - Required: false

==========================
Time Series configuration
==========================
ðŸ”˜ timeseries.timefield



	 - Type: false
	 - Default: STRING
	 - Importance: The name of the top-level field which contains the date in each time series document. Setting this config will create a time series collection where each document will have a BSON date as the value for the timefield.
	 - Required: LOW

ðŸ”˜ timeseries.timefield.auto.convert

Whether to convert the data in the field into a BSON Date format. Supported formats include integer, long, and string.

	 - Type: STRING
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ timeseries.timefield.auto.convert.date.format

The string pattern to convert the source data from. The setting expects the string representation to contain both date and time information and uses the Java DateTimeFormatter.ofPattern(pattern, locale) API for the conversion. If the string only contains date information, then the time since epoch is from the start of that day. If a string representation does not contain time-zone offset, then the setting interprets the extracted date and time as UTC.

	 - Type: STRING
	 - Default: yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	 - Importance: LOW
	 - Required: false

ðŸ”˜ timeseries.timefield.auto.convert.locale.language.tag

The DateTimeFormatter locale language tag to use with the date pattern.

	 - Type: STRING
	 - Default: en
	 - Importance: LOW
	 - Required: false

ðŸ”˜ timeseries.metafield



	 - Type: false
	 - Default: STRING
	 - Importance: The name of the top-level field that contains metadata in each time series document. This field groups related data. It can be of any type except array.
	 - Required: LOW

ðŸ”˜ timeseries.expire.after.seconds

The amount of seconds the data remains in MongoDB before MongoDB deletes it. Omitting this field means data will not be deleted automatically.

	 - Type: INT
	 - Default: 0
	 - Importance: LOW
	 - Required: false

ðŸ”˜ ts.granularity

The expected interval between subsequent measurements for a time-series. Set this to None or leave it empty if the data is not time-series

	 - Type: STRING
	 - Default: None
	 - Importance: LOW
	 - Required: false

==========================
Error handling
==========================
ðŸ”˜ mongo.errors.tolerance

Use this property if you would like to configure the connector's error handling behavior differently from the Connect framework's.

	 - Type: STRING
	 - Default: NONE
	 - Importance: MEDIUM
	 - Required: false

==========================
Consumer configuration
==========================
ðŸ”˜ max.poll.interval.ms

The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).

	 - Type: LONG
	 - Default: 300000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.poll.records

The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.

	 - Type: LONG
	 - Default: 500
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Post Processing
==========================
ðŸ”˜ post.processor.chain

A comma separated list of post processor classes to process the data before saving to MongoDB.

	 - Type: LIST
	 - Default: com.mongodb.kafka.connect.sink.processor.DocumentIdAdder
	 - Importance: LOW
	 - Required: false

ðŸ”˜ field.renamer.mapping

An inline JSON array with objects describing field name mappings. Example: ``[{"oldName":"key.fieldA","newName":"field1"},{"oldName":"value.xyz","newName":"abc"}]``

	 - Type: STRING
	 - Default: []
	 - Importance: LOW
	 - Required: false

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.replace.null.with.default

Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.schemas.enable

Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: all
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

