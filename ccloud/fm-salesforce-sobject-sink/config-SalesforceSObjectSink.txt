==========================
Which topics do you want to get data from?
==========================
ðŸ”˜ topics.regex



	 - Type: false
	 - Default: STRING
	 - Importance: A regular expression that matches the names of the topics to consume from. This is useful when you want to consume from multiple topics that match a certain pattern without having to list them all individually.
	 - Required: LOW

ðŸ”˜ topics



	 - Type: true
	 - Default: LIST
	 - Importance: Identifies the topic name or a comma-separated list of topic names.
	 - Required: HIGH

ðŸ”˜ errors.deadletterqueue.topic.name

The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. Defaults to 'dlq-${connector}' if not set. The DLQ topic will be created automatically if it does not exist. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: dlq-${connector}
	 - Importance: LOW
	 - Required: false

ðŸ”˜ reporter.result.topic.name

The name of the topic to produce records to after successfully processing a sink record. Defaults to 'success-${connector}' if not set. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: success-${connector}
	 - Importance: LOW
	 - Required: false

ðŸ”˜ reporter.error.topic.name

The name of the topic to produce records to after each unsuccessful record sink attempt. Defaults to 'error-${connector}' if not set. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.

	 - Type: STRING
	 - Default: error-${connector}
	 - Importance: LOW
	 - Required: false

==========================
Schema Config
==========================
ðŸ”˜ schema.context.name

Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.

	 - Type: STRING
	 - Default: default
	 - Importance: MEDIUM
	 - Required: false

==========================
Input messages
==========================
ðŸ”˜ input.data.format



	 - Type: true
	 - Default: STRING
	 - Importance: Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR and PROTOBUF. Note that you need to have Confluent Cloud Schema Registry configured
	 - Required: HIGH

==========================
How should we connect to your data?
==========================
ðŸ”˜ connector.class



	 - Type: true
	 - Default: STRING
	 - Importance: 
	 - Required: HIGH

ðŸ”˜ name



	 - Type: true
	 - Default: STRING
	 - Importance: Sets a name for your connector.
	 - Required: HIGH

==========================
Kafka Cluster credentials
==========================
ðŸ”˜ kafka.auth.mode

Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.

	 - Type: STRING
	 - Default: KAFKA_API_KEY
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ kafka.service.account.id



	 - Type: false
	 - Default: STRING
	 - Importance: The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.
	 - Required: HIGH

ðŸ”˜ kafka.api.key



	 - Type: false
	 - Default: STRING
	 - Importance: Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

ðŸ”˜ kafka.api.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.
	 - Required: HIGH

==========================
Batching configurations
==========================
ðŸ”˜ salesforce.enable.batching

Enable Batching Support

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ salesforce.batch.size

Max number of records to send in a single batch to Salesforce (only applicable if batching is enabled)

	 - Type: INT
	 - Default: 200
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ salesforce.all.or.none

To enable All Or None functionality while batching. When true, entire batch fails if any record fails. When false, individual records can fail within a batch.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: HIGH
	 - Required: false

==========================
How should we connect to Salesforce?
==========================
ðŸ”˜ salesforce.grant.type

Salesforce grant type. Valid options are 'PASSWORD', 'CLIENT_CREDENTIALS' and 'JWT_BEARER'.

	 - Type: STRING
	 - Default: PASSWORD
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ salesforce.instance

The URL of the Salesforce endpoint to use. When using 'CLIENT_CREDENTIALS' grant type, provide your Salesforce domain URL. The default is https://login.salesforce.com, which directs the connector to use the endpoint specified in the authentication response.

	 - Type: STRING
	 - Default: https://login.salesforce.com
	 - Importance: HIGH
	 - Required: false

ðŸ”˜ salesforce.username



	 - Type: false
	 - Default: STRING
	 - Importance: The Salesforce username the connector should use.
	 - Required: HIGH

ðŸ”˜ salesforce.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The Salesforce password the connector should use.
	 - Required: HIGH

ðŸ”˜ salesforce.password.token



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The Salesforce security token associated with the username.
	 - Required: HIGH

ðŸ”˜ salesforce.consumer.key



	 - Type: true
	 - Default: PASSWORD
	 - Importance: The client id(consumer key) for the Salesforce Connected app.
	 - Required: HIGH

ðŸ”˜ salesforce.consumer.secret



	 - Type: false
	 - Default: PASSWORD
	 - Importance: The client secret(consumer secret) for the Salesforce Connected app.
	 - Required: MEDIUM

ðŸ”˜ salesforce.jwt.keystore.file



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Salesforce JWT keystore file which contains the private key.
	 - Required: MEDIUM

ðŸ”˜ salesforce.jwt.keystore.password



	 - Type: false
	 - Default: PASSWORD
	 - Importance: Password used to access JWT keystore file.
	 - Required: MEDIUM

ðŸ”˜ salesforce.object.num

The number of salesforce objects to write to. This is used when multiple SObjects are enabled.

	 - Type: INT
	 - Default: 1
	 - Importance: HIGH
	 - Required: false

==========================
Object 1 configuration
==========================
ðŸ”˜ salesforce.object1



	 - Type: false
	 - Default: STRING
	 - Importance: The Salesforce SObject1 to write to.
	 - Required: HIGH

ðŸ”˜ salesforce.object1.topics



	 - Type: false
	 - Default: LIST
	 - Importance: The comma separated list of topics associated with Salesforce SObject1
	 - Required: HIGH

ðŸ”˜ salesforce.object1.override.event.type

A flag to indicate that the Kafka SObject1 source record EventType(create, update, delete) is overridden to use the operation specified in the SObject1 override operation field. 

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object1.sink.operation

The Salesforce sink operation to perform on the SObject1. One of: insert, update, upsert, delete. Default is insert. This feature works if ``override.event.type`` is true.

	 - Type: STRING
	 - Default: insert
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object1.ignore.fields



	 - Type: false
	 - Default: LIST
	 - Importance: Comma separate list of fields from the source Kafka record to ignore when pushing a record into Salesforce.
	 - Required: LOW

ðŸ”˜ salesforce.object1.ignore.reference.fields

Flag to prevent reference type fields from being updated or inserted in Salesforce SObjects.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object1.use.custom.id.field

Flag to use custom external id field in SObjects.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object1.custom.id.field.name



	 - Type: false
	 - Default: STRING
	 - Importance: Name of a custom external id field in SObject to structure Rest Api calls for insert, upsert, delete, and update operations.
	 - Required: LOW

==========================
Object 2 configuration
==========================
ðŸ”˜ salesforce.object2



	 - Type: false
	 - Default: STRING
	 - Importance: The Salesforce SObject2 to write to.
	 - Required: HIGH

ðŸ”˜ salesforce.object2.topics



	 - Type: false
	 - Default: LIST
	 - Importance: The comma separated list of topics associated with Salesforce SObject1
	 - Required: HIGH

ðŸ”˜ salesforce.object2.override.event.type

A flag to indicate that the Kafka SObject2 source record EventType(create, update, delete) is overridden to use the operation specified in the SObject2 override operation field. 

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object2.sink.operation

The Salesforce sink operation to perform on the SObject2. One of: insert, update, upsert, delete. Default is insert. This feature works if ``override.event.type`` is true.

	 - Type: STRING
	 - Default: insert
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object2.ignore.fields



	 - Type: false
	 - Default: LIST
	 - Importance: Comma separate list of fields from the source Kafka record to ignore when pushing a record into Salesforce.
	 - Required: LOW

ðŸ”˜ salesforce.object2.ignore.reference.fields

Flag to prevent reference type fields from being updated or inserted in Salesforce SObjects.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object2.use.custom.id.field

Flag to use custom external id field in SObjects.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object2.custom.id.field.name



	 - Type: false
	 - Default: STRING
	 - Importance: Name of a custom external id field in SObject to structure Rest Api calls for insert, upsert, delete, and update operations.
	 - Required: LOW

==========================
Object 3 configuration
==========================
ðŸ”˜ salesforce.object3



	 - Type: false
	 - Default: STRING
	 - Importance: The Salesforce SObject3 to write to.
	 - Required: HIGH

ðŸ”˜ salesforce.object3.topics



	 - Type: false
	 - Default: LIST
	 - Importance: The comma separated list of topics associated with Salesforce SObject3
	 - Required: HIGH

ðŸ”˜ salesforce.object3.override.event.type

A flag to indicate that the Kafka SObject3 source record EventType(create, update, delete) is overridden to use the operation specified in the SObject3 override operation field.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object3.sink.operation

The Salesforce sink operation to perform on the SObject3. One of: insert, update, upsert, delete. Default is insert. This feature works if override.event.type is true.

	 - Type: STRING
	 - Default: insert
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object3.ignore.fields



	 - Type: false
	 - Default: LIST
	 - Importance: Comma separate list of fields from the source Kafka record to ignore when pushing a record into Salesforce.
	 - Required: LOW

ðŸ”˜ salesforce.object3.ignore.reference.fields

Flag to prevent reference type fields from being updated or inserted in Salesforce SObjects.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object3.use.custom.id.field

Flag to use custom external id field in SObjects.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object3.custom.id.field.name



	 - Type: false
	 - Default: STRING
	 - Importance: Name of a custom external id field in SObject to structure Rest Api calls for insert, upsert, delete, and update operations.
	 - Required: LOW

==========================
Object 4 configuration
==========================
ðŸ”˜ salesforce.object4



	 - Type: false
	 - Default: STRING
	 - Importance: The Salesforce SObject4 to write to.
	 - Required: HIGH

ðŸ”˜ salesforce.object4.topics



	 - Type: false
	 - Default: LIST
	 - Importance: The comma separated list of topics associated with Salesforce SObject4
	 - Required: HIGH

ðŸ”˜ salesforce.object4.override.event.type

A flag to indicate that the Kafka SObject4 source record EventType(create, update, delete) is overridden to use the operation specified in the SObject4 override operation field.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object4.sink.operation

The Salesforce sink operation to perform on the SObject4. One of: insert, update, upsert, delete. Default is insert. This feature works if override.event.type is true.

	 - Type: STRING
	 - Default: insert
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object4.ignore.fields



	 - Type: false
	 - Default: LIST
	 - Importance: Comma separate list of fields from the source Kafka record to ignore when pushing a record into Salesforce.
	 - Required: LOW

ðŸ”˜ salesforce.object4.ignore.reference.fields

Flag to prevent reference type fields from being updated or inserted in Salesforce SObjects.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object4.use.custom.id.field

Flag to use custom external id field in SObjects.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object4.custom.id.field.name



	 - Type: false
	 - Default: STRING
	 - Importance: Name of a custom external id field in SObject to structure Rest Api calls for insert, upsert, delete, and update operations.
	 - Required: LOW

==========================
Object 5 configuration
==========================
ðŸ”˜ salesforce.object5



	 - Type: false
	 - Default: STRING
	 - Importance: The Salesforce SObject5 to write to.
	 - Required: HIGH

ðŸ”˜ salesforce.object5.topics



	 - Type: false
	 - Default: LIST
	 - Importance: The comma separated list of topics associated with Salesforce SObject5
	 - Required: HIGH

ðŸ”˜ salesforce.object5.override.event.type

A flag to indicate that the Kafka SObject5 source record EventType(create, update, delete) is overridden to use the operation specified in the SObject5 override operation field.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object5.sink.operation

The Salesforce sink operation to perform on the SObject5. One of: insert, update, upsert, delete. Default is insert. This feature works if override.event.type is true.

	 - Type: STRING
	 - Default: insert
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object5.ignore.fields



	 - Type: false
	 - Default: LIST
	 - Importance: Comma separate list of fields from the source Kafka record to ignore when pushing a record into Salesforce.
	 - Required: LOW

ðŸ”˜ salesforce.object5.ignore.reference.fields

Flag to prevent reference type fields from being updated or inserted in Salesforce SObjects.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object5.use.custom.id.field

Flag to use custom external id field in SObjects.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ salesforce.object5.custom.id.field.name



	 - Type: false
	 - Default: STRING
	 - Importance: Name of a custom external id field in SObject to structure Rest Api calls for insert, upsert, delete, and update operations.
	 - Required: LOW

==========================
Connection details
==========================
ðŸ”˜ connection.timeout

The amount of time to wait in milliseconds while connecting to the Salesforce streaming endpoint.

	 - Type: LONG
	 - Default: 30000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ behavior.on.api.errors

Error handling behavior config for any API errors.

	 - Type: STRING
	 - Default: ignore
	 - Importance: LOW
	 - Required: false

ðŸ”˜ request.max.retries.time.ms

In case of error when making a request to Salesforce, the connector will retry until this time (in ms) elapses. The default value is 30000 (30 seconds). Minimum value is 1 sec

	 - Type: LONG
	 - Default: 30000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ behavior.on.null.record

Error handling behavior config for null records

	 - Type: STRING
	 - Default: fail
	 - Importance: LOW
	 - Required: false

==========================
Consumer configuration
==========================
ðŸ”˜ max.poll.interval.ms

The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).

	 - Type: LONG
	 - Default: 300000
	 - Importance: LOW
	 - Required: false

ðŸ”˜ max.poll.records

The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.

	 - Type: LONG
	 - Default: 500
	 - Importance: LOW
	 - Required: false

==========================
Number of tasks for this connector
==========================
ðŸ”˜ tasks.max



	 - Type: true
	 - Default: INT
	 - Importance: Maximum number of tasks for the connector.
	 - Required: HIGH

==========================
Additional Configs
==========================
ðŸ”˜ value.converter.reference.subject.name.strategy

Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.

	 - Type: STRING
	 - Default: DefaultReferenceSubjectNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ errors.tolerance

Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.

	 - Type: STRING
	 - Default: all
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.ignore.default.for.nullables

When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.

	 - Type: BOOLEAN
	 - Default: false
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.decimal.format

Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:

	 - Type: STRING
	 - Default: BASE64
	 - Importance: LOW
	 - Required: false

ðŸ”˜ value.converter.connect.meta.data



	 - Type: false
	 - Default: BOOLEAN
	 - Importance: Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.
	 - Required: LOW

ðŸ”˜ value.converter.value.subject.name.strategy

Determines how to construct the subject name under which the value schema is registered with Schema Registry.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

ðŸ”˜ key.converter.key.subject.name.strategy

How to construct the subject name for key schema registration.

	 - Type: STRING
	 - Default: TopicNameStrategy
	 - Importance: LOW
	 - Required: false

==========================
Auto-restart policy
==========================
ðŸ”˜ auto.restart.on.user.error

Enable connector to automatically restart on user-actionable errors.

	 - Type: BOOLEAN
	 - Default: true
	 - Importance: MEDIUM
	 - Required: false

